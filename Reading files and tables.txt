All are read and stored as dataframe.

Reading csv file : spark.read
   .option("header", "true")       # Use first line of all files as header
   .option("sep", "\t")            # Use tab delimiter (default is comma-separator)
   .option("inferSchema", "true")
   .csv('filepath')

Reading json file : spark.read
                    .option("inferschema","true")
                    .json('filepath')

reading parquet file : spark.read.parquet('filepath')

reading data from table : spark.read.table('tablename')
-----------------------------------------------------------------------------------------------
User defined schema:
CSV:

from pyspark.sql.types import *

csvSchema = StructType([
  StructField("timestamp", StringType(), False),
  StructField("site", StringType(), False),
  StructField("requests", IntegerType(), False)
])

df = spark.read
   .option("header", "true")       # Use first line of all files as header
   .option("sep", "\t")            # Use tab delimiter (default is comma-separator)
   .schema(csvSchema)